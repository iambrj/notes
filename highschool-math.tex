\documentclass[titlepage, 12pt]{book}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
}

\newtcbtheorem[]{definition}{Definition}%
{colback=magenta!5,colframe=magenta!100!black,fonttitle=\bfseries}{th}

\newtcbtheorem[]{proposition}{Proposition}%
{colback=cyan!5,colframe=cyan!100!black,fonttitle=\bfseries}{th}

\newtcbtheorem[]{theorem}{Theorem}%
{colback=orange!5,colframe=orange!100!black,fonttitle=\bfseries}{th}

\begin{document}

\title{Notes on high school mathematics}
\author{Bharathi Ramana Joshi\\\url{https://github.com/iambrj/notes}}
\maketitle

\tableofcontents

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

I had learnt a lot of useful (and \textit{fun}!) mathematics in my high school,
but all my notes back then were handwritten. This document is an attempt at
digitalizing them since I still find some of that math useful and having a
digital document makes it easier to revisit material.

\chapter{Functions}
\begin{definition}{Cartesian product}{}
    If $A$ and $B$ are two sets, then their \textit{Cartesian product} is
    defined as the set $\{(a, b)\mid a\in A,b\in B\}$ denoted by $A\times B$
\end{definition}

\begin{definition}{Relation}{}
    If $A$ and $B$ are two sets, then any subset of $A\times B$ is called a
    \textit{relation} from $A$ to $B$
\end{definition}

\begin{definition}{Function}{}
    If $A$ and $B$ are two sets, then function $f$ from $A$ to $B$ is a relation
    from $A$ to $B$ such that $\forall a\in A,\exists b\in B$ such that $(a,
    b)\in f$. It is denoted by $f:A\rightarrow B$, the set $A$ is called the
    \textit{domain} and $b$ is called the \textit{co-domain} of $f$
\end{definition}

\begin{definition}{Image, Pre-image}{}
    If $f:A\rightarrow B$ is a function such that $f(a) = b$, then $b$ is called
    the \textbf{image} of $a$ under $f$ and $a$ is called the pre-image of $b$
    under $f$
\end{definition}

\begin{definition}{Range}{}
    If $f:A\rightarrow B$ is a function then $f(A)$, the set of all images is
    called the range of $f$
\end{definition}

\begin{definition}{Injection}{}
    A function $f:A\rightarrow B$ is called an
    \textbf{injection}/\textbf{one-to-one} if distinct elements of $A$ have
    distinct images in $B$
\end{definition}

\begin{definition}{Surjection}{}
    A function $f:A\rightarrow B$ is called an
    \textbf{surjection}/\textbf{onto} if range is equal to the co-domain
\end{definition}

\begin{definition}{Bijection}{}
    A function $f:A\rightarrow B$ is called an \textbf{bijection} if it is both
    into and onto
\end{definition}

\begin{definition}{Function equality}{}
    Two functions $f$ and $g$ are equal, iff
    \begin{enumerate}
        \item Both have same domains
        \item Images of all elements from the domain are the same, i.e. $f(x) =
            g(x), \forall x\in $ domain
    \end{enumerate}
\end{definition}

\begin{definition}{Inverse function}{}
    If $f:A\rightarrow B$ is a bijection, then the relation $f^{-1} = \{(b,
    a)\mid f(a) = b\}$ is defined as the inverse function of $f$
\end{definition}

\begin{definition}{Function composition}{}
    If $f : A\rightarrow B$ and $g : B\rightarrow C$ are two functions, then
    their composition function $f\circ g$ is defined as the relation $\{(a,
    g(f(a)))\mid a\in A\}$
\end{definition}

\chapter{Matrices}

\begin{definition}{Matrix}{}
    An ordered rectangular array of elements is called a \textbf{matrix}
\end{definition}

\begin{definition}{Order of a Matrix}{}
    A matrix having $m$ rows and $n$ columns is said to be of order $m\times n$,
    read a s$m$ cross $n$ or $m$ by $n$
\end{definition}

Some common matrices
\begin{enumerate}
    \item\textbf{Square matrix} : $m = n$
    \item\textbf{Diagonal matrix} : All non-diagonal elements of a square matrix
        are zeroes
    \item\textbf{Scalar matrix} : Diagonal matrix where all diagonal elements
        are equal
    \item\textbf{Unity/Identity matrix} : Diagonal matrix where all diagonal
        elements are 1
    \item\textbf{Null/Zero matrix} : All elements are zeroes
    \item\textbf{Row/Column matrix} : Matrix with only single row/column
    \item\textbf{Triangular matrix} : All elements below/above (lower/upper) are
        zeroes
\end{enumerate}

\begin{definition}{Equality of matrices}{}
    Two matrices are equal if they have same order and corresponding elements
    are equal
\end{definition}

\begin{definition}{Sum of two matrices}{}
    If two matrices are of equal order then their sum matrix is defined as sum
    of corresponding elements
\end{definition}

\begin{definition}{Scalar multiple of a matrix}{}
    The scalar multiple of a matrix is defined as the matrix obtained by
    multiplying each element by fixed scalar
\end{definition}

\begin{definition}{Product of two matrices}{}
    If $A = [a_{ik}]_{m\times n}$ and $B = [b_{kj}]_{n\times p}$ are two
    matrices, then their product $C = [c_{ij}]_{m\times p}$ is defined as
    $c_{ij} = \sum_{k = 1}^n a_{ik}b_{kj}$
\end{definition}

\begin{definition}{Transpose of a matrix}{}
    The matrix obtained by interchanging the rows and columns is called the
    transpose matrix. Transpose of a matrix $A$ is denoted by $A^\prime$.
\end{definition}

\begin{definition}{Symmetric matrix}{}
    A matrix is Symmetric if it is equal to its transpose, i.e. $A = A^\prime$
\end{definition}

\begin{definition}{Skew-symmetric matrix}{}
    A matrix is skew-symmetric if $A^\prime =-A$
\end{definition}

\begin{definition}{Determinant}{}
    Since $a_1b_2 - a_2b_1$ determines whether below system of linear equations
    has unique solution ($\neq 0$) or not (= 0)
    \begin{align*}
        a_1x+b_1y = c_1\\
        a_2x+b_2y = c_2
    \end{align*}
\end{definition}

\begin{definition}{Minor}{}
    For a square matrix, the minor of an element is defined as the determinant
    of the smaller matrix obtained by deleting the rows and columns of that
    element.
\end{definition}

\begin{definition}{Cofactor}{}
    Cofactor of element $ij$ is defined as the minor multiplied by $-1^{(i +
    j)}$
\end{definition}

\begin{definition}{Singular matrix}{}
    A matrix is singular if its determinant is zero
\end{definition}

\begin{definition}{Adjoint}{}
    \begin{align*}
        \begin{bmatrix}
            a_1 & b_1 & c_1\\
            a_2 & b_2 & c_2\\
            a_3 & b_3 & c_3\\
        \end{bmatrix}
        = 
        \begin{bmatrix}
            A_1 & A_2 & A_3\\
            B_1 & B_2 & B_3\\
            C_1 & C_2 & C_3\\
        \end{bmatrix}
    \end{align*}
    uppercase letters are cofactors of corresponding lowercase letters
\end{definition}

\begin{definition}{Inverse matrix}{}
    A matrix $A$ is \textbf{\textit{invertible}} if $|A|\neq 0, A^{-1} =
    \frac{Adj(A)}{|A|}$
\end{definition}

\begin{definition}{Rank}{}
    Rank of a matrix is the maximum order of the non-singular square
    submatrices.
\end{definition}

Rank is invariant under
    \begin{enumerate}
        \item Interchanging two rows/columns
        \item Multiplying rows/columns by a non-zero number
        \item Adding elements of a row/column to the corresponding elements of
            another row/column
    \end{enumerate}

\begin{theorem}{Solubility of system of equations}{}
    A system of three equations in three unknowns $AX = D$ has
    \begin{enumerate}
        \item \textbf{\textit{Unique}} solutions if $rank(A) = rank([A\;D]) = 3$
        \item \textbf{\textit{Infinite}} solutions if $rank(A) = rank([A\;D]) < 3$
        \item \textbf{\textit{Zero}} solutions if $rank(A)\neq rank([A\; D])$
    \end{enumerate}
\end{theorem}

\begin{definition}{Cramer's rule}{}
    \textbf{\textit{Cramer's rule}} states that
    \begin{align*}
        \frac{x}{\Delta_1} = \frac{y}{\Delta_2} = \frac{z}{\Delta_2} = \frac{1}{\Delta}
    \end{align*}
    where
    \begin{enumerate}
        \item 
            \begin{align*}
                \Delta =
                \begin{vmatrix}
                    a_1 & b_1 & c_1\\
                    a_2 & b_2 & c_2\\
                    a_3 & b_3 & c_3\\
                \end{vmatrix}
            \end{align*}
        \item 
            \begin{align*}
                \Delta_1 =
                \begin{vmatrix}
                    d_1 & b_1 & c_1\\
                    d_2 & b_2 & c_2\\
                    d_3 & b_3 & c_3\\
                \end{vmatrix}
            \end{align*}
        \item 
            \begin{align*}
                \Delta_2 =
                \begin{vmatrix}
                    a_1 & d_1 & c_1\\
                    a_2 & d_2 & c_2\\
                    a_3 & d_3 & c_3\\
                \end{vmatrix}
            \end{align*}
        \item 
            \begin{align*}
                \Delta_3 =
                \begin{vmatrix}
                    a_1 & b_1 & d_1\\
                    a_2 & b_2 & d_2\\
                    a_3 & b_3 & d_3\\
                \end{vmatrix}
            \end{align*}
    \end{enumerate}
\end{definition}

\begin{definition}{Matrix inversion method}{}
    The matrix inversion method to solve the system of equations $AX = D$ is $X
    = A^{-1}D$
\end{definition}

\begin{definition}{Gauss-Jordan method}{}
    To solve a system of equations using Gauss-Jordan method, we transform
    \begin{align*}
        \begin{bmatrix}
            a_1 & b_1 & c_1 & d_1\\
            a_2 & b_2 & c_2 & d_2\\
            a_3 & b_3 & c_3 & d_3\\
            a_4 & b_4 & c_4 & d_4\\
        \end{bmatrix}
        \rightarrow 
        \begin{bmatrix}
            1 & 0 & 0 & \alpha\\
            0 & 1 & 0 & \beta\\
            0 & 0 & 1 & \gamma\\
        \end{bmatrix}
    \end{align*}
    via a series of rank invariant transformations
\end{definition}
Note
\begin{enumerate}
    \item If matrix can be transformed into above form it has unique solution $x
        = \alpha, y = \beta, z = \gamma$
    \item If one entire row is zeroes, then system has infinite solutions
    \item If row of $A$ is zero, but $D$'s element is non-zero, system has no
        solutions
\end{enumerate}

\begin{theorem}{}{}
    The homogeneous system of equations $AX = O$ has
    \begin{enumerate}
        \item only the trivial solution, if $rank(A) = 3$
        \item infinite solutions, otherwise
    \end{enumerate}
\end{theorem}

% TODO : Vectors chapters

\chapter{Trigonometry}

\begin{theorem}{Degrees, gradients and radians}{}
    \begin{align*}
    \frac{180}{D} = \frac{200}{G} = \frac{\pi}{R}
    \end{align*}
\end{theorem}

\begin{definition}{Trigonometric ratios}{}
    \begin{enumerate}
        \item $sin\theta = \frac{O}{H}$
        \item $cos\theta = \frac{A}{H}$
        \item $tan\theta = \frac{O}{A}$
        \item $cosec\theta = \frac{H}{O}$
        \item $sec\theta = \frac{H}{A}$
        \item $cot\theta = \frac{A}{O}$
    \end{enumerate}
\end{definition}

\begin{theorem}{Identities}{}
    \begin{enumerate}
        \item $cos^2\theta + sin^2\theta = 1$
        \item $1 + tan^2\theta = sec^2\theta$
        \item $1 + cot^2\theta = cosec^2\theta$
    \end{enumerate}
\end{theorem}

\begin{theorem}{Common angle values}{}

    \begin{center}
        \begin{tabular}{ |c|c|c|c|c|c| }
            \hline
            Angle/ratio & 0 & 30 & 45 & 60 & 90 \\
            \hline
            sin & 0 & $\frac{1}{2}$ & $\frac{1}{\sqrt{2}}$ & $\frac{\sqrt{3}}{2}$ & 1 \\
            \hline
            cos & 1 & $\frac{\sqrt{3}}{2}$ & $\frac{1}{\sqrt{2}}$ & $\frac{1}{2}$ & 0 \\
            \hline
            tan & 0 & $\frac{1}{\sqrt{3}}$ & 1 & $\sqrt{3}$ & $\infty$ \\
            \hline
        \end{tabular}
    \end{center}
\end{theorem}

\begin{theorem}{Compound angle identities}{}
    \begin{enumerate}
        \item cos (A + B) = cos A cos B - sin A sin B
        \item sin (A + B) = sin A cos B + cos A sin B
        \item tan (A + B) = $\frac{tan A + tan B}{1 - tan A tan B}$
        \item cot (A + B) = $\frac{cot A cot B - 1}{cot A + cot B}$
        \item sin (A + B) sin (A - B) = sin$^2$ A - sin$^2$ B
        \item cos (A + B) cos (A - B) = cos$^2$ A - sin$^2$ B
        \item sin (A + B + C) = ($\Sigma$sin A cos B cos C) - sin A sin B sin C
        \item cos (A + B + C) = cos A cos B cos C - $\Sigma$ cos A sin B sin C
        \item tan (A + B + C) = $\frac{\Sigma tan A - \Pi tan A}{1 - \Sigma tan A tan B}$
        \item cot (A + B + C) = $\frac{\Sigma cot A - \Pi cot A}{1 - \Sigma cot A cot B}$
    \end{enumerate}
\end{theorem}

\begin{theorem}{Peak points of an expression}{}
    The expression $asinx + bcosx + c$ has the maximum value $c +
    \sqrt{a^2+b^2}$ and minimum value $c - \sqrt{a^2+b^2}$
\end{theorem}

\begin{theorem}{Multiple angle identities}{}
    \begin{enumerate}
        \item sin 2A = 2sin A cos A = $\frac{2tan A}{1 + tan^2A}$
        \item cos 2A = cos$^2$A - sin$^2$A = $\frac{1 - tan^2 A}{1 + tan^2A}$
        \item tan 2A = $\frac{2tanA}{1 - tan^2A}$
        \item sin 3A = 3 sin A - 4 sin$^3$ A
        \item cos 3A = 4cos$^3$A - 3 cos A
        \item tan 3A = $\frac{3tanA - tan^3A}{1 - 3 tan^2A}$
        \item cot 3A = $\frac{3cot A - cot^3 A}{1 - 3 cot^2A}$
    \end{enumerate}
\end{theorem}

\begin{theorem}{Sum and product transformations}{}
    \begin{enumerate}
        \item 2 sin A cos B = sin (A + B) + sin (A - B)
        \item 2 cos A sin B = sin (A + B) - sin (A - B)
        \item 2 cos A cos B = cos (A + B) + cos (A - B)
        \item 2 sin A sin B = cos (A + B) - cos (A - B)
    \end{enumerate}
\end{theorem}

\chapter{Limits and Continuity}

\begin{definition}{Neighbourhood}{}
    If $a\in\mathbb{R}$ and $\delta\in\mathbb{R}, \delta > 0$, then the interval
    $(a - \delta, a + \delta)$ is called the $\delta$-neighbourhood of $a$
\end{definition}

\begin{definition}{Limit point}{}
    Let $E\subseteq\mathbb{R}$, $f:E\rightarrow\mathbb{R}$ and let $a$ be such
    that $((a - r, a + r)\\ \{a\})\cap E$ is nonempty for each $r>0$ (i.e. $f$
    is well defined in the deleted $r$ neighbourhood of $a$). Then, if
    $\forall\epsilon > 0$, $\exists\delta > 0$ such that $|f(x) - l| <
    \epsilon$ whenever $|x - a| < \delta$, $l$ is said to be a limit point of
    $f$ at $a$. This is written as
    \begin{align*}
        \lim_{x\rightarrow a} f(x) = l
    \end{align*}
\end{definition}

\begin{theorem}{Sandwich theorem}{}
    Let $E\subset\mathbb{R}$, $f,g,h : E\rightarrow\mathbb{R}$ and
    $a\in\mathbb{R}$ such that $\lim_{x\rightarrow a}f(x) = l =
    \lim_{x\rightarrow a} h(x)$ and $f(x)\leq g(x)\leq h(x)$, for all $x\in E$
    then $\lim_{x\rightarrow a}g(x)$ exists and is equal to $l$
\end{theorem}

\begin{definition}{Continuity}{}
    A function $f:E(\subset\mathbb{R})\rightarrow\mathbb{R}$ is continuous at
    $a$ if for all $\epsilon > 0$ there exists a $\delta > 0$ such that whenever
    $|x - a| < \delta$, we have $|f(x) -f(a)| < \epsilon$
\end{definition}

\begin{definition}{Left \& Right Continuity}{}
    A function $f:E(\subset\mathbb{R})\rightarrow\mathbb{R}$ that is well
    defined in the deleted $r$ neighbourhood of $a, \forall a in E$ is right
    continuous at $a$ if $\lim_{x\rightarrow a+}f(x)$ exists and is equal to
    $f(a)$ and left continuous at $a$ if $\lim_{x\rightarrow a-}f(x)$ exists and
    is equal to $f(a)$.
\end{definition}

A few useful results:
\begin{enumerate}
    \item If $f$ and $g$ are continuous at $a$, then $f + g$, $f - g$, $fg$, and
        $cf$ are all also continuous at $a$. Additionally, if $g(a)\neq 0$ then
        $\frac{f}{g}$ is continuous at $a$.
    \item If $f:A\rightarrow R$ and $g:B\rightarrow R$ are two continuous
        functions, then their composition $f\circ g$ is also continuous.
\end{enumerate}

\chapter{Differentiation}
If $f:I\rightarrow R$ is a function such that $\lim_{h\rightarrow 0}\frac{f(a +
h) - f(a)}{h}$ exists, then $f$ is said to be differentiable at $a$ and the
limit is called the derivative of $f$ at $a$. In other words,
\begin{align*}
    f'(a) &= \lim_{h\rightarrow 0}\frac{f(a + h) - f(a)}{h}
          &= \lim_{x\rightarrow a}\frac{f(x) - f(a)}{x - a}
\end{align*}
is the derivative of $f$ at $a$.

\textbf{Note:} If $f$ is differentiable at $a$ (i.e. it is \textit{smooth}),
then it is continuous at $a$. However, if $f$ is continuous at $a$, it need not
be continuous at $a$ (e.g. $f(x) = |x|$ is continuous but not differentiable at
$x = 0$).

\begin{theorem}{Derivative of product of functions}{}
    $(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$.
\end{theorem}

\begin{theorem}{Derivative of quotient of functions}{}
    $(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$.
\end{theorem}

\begin{theorem}{Derivative of composite functions}{}
    $(f\circ g(x))' = f'(g(x))g'(x)$.
\end{theorem}

\end{document}

